## Задания до 28 марта (ч.1)
1) Написать mpi программу, печатающую "Hello, World!". Запустить программу на 4 процессах. 

2) Написать  программу, печатающую номер процесса и общее количество процессов в коммуникаторе MPI_COMM_WORLD.

3) Написать программу, запустить ее на 2х процессах. На нулевом процессе задать массив а из 10 элементов, значения сгенерировать случайным образом. Переслать этот массив целиком первому процессу с помощью функции MPI_Send. Распечатать на первом процессе принятый массив.

4) Используя блокирующую операцию передачи сообщений (MPI_Send и MPI_Recv) выполнить пересылку данных одномерного массива, из процесса с номером 1 на остальные процессы группы. На процессах получателях до выставления функции приема (MPI_Recv) определить количество принятых данных (MPI_Probe). Выделить память под размер приемного буфера, после чего вызвать функцию MPI_Recv. Полученное сообщение выдать на экран.

5) Написать программу и запустить ее на p (= 2, 3, 4, 6, 8) процессах. На нулевом процессе объявить и задать массив из 12 элементов. С помощью функции MPI_Send разослать блоки массива на остальные процессы. Размер блока массива 12/p+1. В результате на нулевом процессе должны быть элементы массива с 0 до 12/p+1, на первом процессе с 12/p+1 до 2×(12/p+1), на 3м процессе с 2×(12/p+1) до 3×(12/p+2) и т.д. Вывести элементы массива на экран на каждом процессе.

6) (*) Написать программу, вычисляющую элементы вектора z  по формуле zi=xi+yi. Векторы х, у задаются на нулевом процессе и равными блоками пересылаются остальным процессам, , - заданные числа. Пересылка данных осуществляется функцией MPI_Send. Все процессы по формуле вычисляют свои элементы массива z. Каждый процесс отправляет на нулевой процесс посчитанные элементы.

## До 4 апреля (ч.2)
1) Пусть векторы х, у заданы на нулевом процессе. Написать программу, в которой векторы равными блоками с нулевого процесса пересылаются остальным процессам. Пересылка данных осуществляется функцией MPI_Send. Все процессы по формуле вычисляют свои элементы искомых массивов. Каждый процесс отправляет на нулевой процесс подсчитанные элементы массивов. В программе реализовать следующие операции вычисления векторов:
a. zi=xi+yi, где , - заданные числа;
b. yi=xi+yi, где , - заданные числа;
c. zi=xi*yi;
d. Обмен элементов двух векторов yi xi.
2) Пусть матрицы вещественных чисел A, B заданы на нулевом процессе. Написать программу, в которой матрицы равными кусками с нулевого процесса пересылаются остальным процессам. Пересылка данных осуществляется функцией MPI_Send. Все процессы по формуле вычисляют свои элементы искомых матриц. Каждый процесс отправляет на нулевой процесс подсчитанные элементы матриц. В программе реализовать следующие операции вычисления матриц:
a. C = A * B   (C[i][j] = A[i][j] * B[i][j] ) - поэлементное умножение;
b. C = A * B   -  умножение матриц;
c. B = AT       	-  транспонирование матрицы;
d. (*) T = gauss(A)    - приведение матрицы к нижнему треугольному виду методом исключения Гаусса

3) Пусть на нулевом процессе задана матрица А и вектор х. Написать программу, в которой вектор и матрица равными блоками с нулевого процесса пересылаются остальным процессам. Пересылка данных осуществляется функцией MPI_Send. Все процессы по формуле вычисляют свои элементы искомых массивов. Каждый процесс отправляет на нулевой процесс подсчитанные элементы массивов. В программе реализовать следующие матрично-векторные операции:
 x = diag(A)  - выделение главной диагонали;
z = A*x   - умножение матрицы на вектор.

4) (*) Разработать программу, в которой два процесса многократно обмениваются сообщениями длиной n байт. Выполнить эксперименты и оценить зависимость времени выполнения операции от длины сообщения. Рассмотреть два варианта обмена сообщениями
PingPong  -  двунаправленная передача. Выполняется передача данных от одного процесса другому с последующим возвратом переданных данных в исходный процесс. Время выполнение замеряется после получения обратного ответа на исходном процессе.
PingPing - однонаправленная передача. Два процесса одновременно выполняют передачу данных друг другу. Время выполнение замеряется после получения сообщения каждым процессом.

## До 11 апреля (ч.3)
1) Написать программу вычисления нормы ||x||1=i=0n-1abs(xi)вектора x. Для распределения элементов вектора x по процессам использовать функцию MPI_Scatter. Для получения итоговой суммы на нулевом процессе использовать функцию MPI_Reduce с операцией MPI_Sum.
2) Написать программу вычисления скалярного произведения двух векторов(x,y)=i=0n-1xiyi. Для распределения элементов вектора x по процессам использовать функцию MPI_Scatter. Для получения итоговой суммы на нулевом процессе использовать функцию MPI_Reduce с операцией MPI_Sum.
3) В массиве вещественных чисел вычислить минимальное значение и глобальный индекс минимального элемента. Использовать функции MPI_Scatter и MPI_Reduce с операцией MPI_MINLOC.

## До 18 апреля (ч.4)
1) Написать программу вычисления нормы матрицы А по формуле s=j=0..m(i=0nabs(a[i][j])), где m- число столбцов, n - число строк. Для распределения элементов матрицы А по процессам использовать функцию MPI_Scatter. Для получения итогового значения использовать функцию MPI_Reduce с операцией MPI_MAX.
2) Написать программу вычисления поэлементного умножения матриц Сi,j=Ai,jBi.j. Использовать функции MPI_Scatter для распределения элементов матриц A и B, и MPI_Gather для сбора вычисленных данных в матрицу C.
3) Написать программу умножения матрицы на вектор z=Ax. Распределить матрицу А равномерно вдоль строк, вектор х передать всем процессам. После выполнения операции умножения матрицы на вектор на каждом процессе необходимо выполнить сбор полученных локальный частей результирующего вектора на один из процессов. Использовать функцию MPI_Bcast для рассылки вектора x, для распределения элементов матрицы использовать функцию MPI_Scatter, для сбора локальных частей результирующего вектора в глобальный вектор использовать функцию MPI_Gather.

## До 25 апреля (ч.5) 
1) Реализовать при помощи операций MPI_Send и MPI_Recv коллективный сбор данных - операцию MPI_Gather. Обратите внимание, функция MPI_Gather производит сборку блоков данных, посылаемых всеми процессами группы, в один массив процесса с номером root. Длина блоков предполагается одинаковой. Объединение происходит в порядке увеличения номеров процессов-отправителей. То есть данные, посланные процессом i из своего буфера sendbuf, помещаются в i-ю порцию буфера recvbuf процесса root. Длина массива, в который собираются данные, должна быть достаточной для их размещения.
2) Пусть A(n,m) – матрица, созданная на нулевом процессе. Пусть есть  4 процесса и процесс 0 посылает части этой матрицы другим процессам. Процессор 1 получает A(i,j) для i=n/2+1,...,n, и j=1,...,m/2. Процессор 2 получает A(i,j) для i=1,...,n/2 и j=m/2+1,...,m и процессор 3 получает A(i,j) для i=n/2+1,...,n and j=m/2,...,m . Это двумерная декомпозиция А на четыре процесса. Написать программу рассылки частей матрицы по процессам, используя функцию MPI_Scatterv.

На 0 процесс
На 2 процесс
На 1 процесс
На 3 процесс

| На 0 процесс  |  На 2 процесс |
|---|---|---|---|---|
|  На 1 процесс | 
На 3 процесс  |   
3) Предположим, что на каждом процессе есть свое число х. Необходимо найти сумму всех элементов х и сумму разослать всем процессам. Можно использовать связку двух функций: MPI_Reduce и MPI_Bcast, т.е. сначала результат операции редукции разместить на одном процессе, а потом с помощью функции MPI_Bcast разослать его всем остальным процессам. Можно использовать функцию MPI_Allreduce, которая аналогична функции MPI_Reduce, но сохраняет результат редукции в адресном пространстве всех процессов. Написать программу для измерения времени, необходимого для выполнения функции MPI_Allreduce на MPI_COMM_WORLD. Как изменяется время работы функции MPI_Allreduce при изменении размера коммуникатора? Насколько быстрее работает MPI_Allreduce чем связка двух функций MPI_Reduce и MPI_Bcast?

## (ч.6)
1) На процессе с номером 0 объявить и заполнить значениями матрицу a[8][8]. Создать новый тип данных для рассылки нечетных строк матрицы а в матрицу b[4][8], и для рассылки четных строк матрицы в матрицу c[4][8], используя функцию MPI_Type_vector.
2) Создать тип данных с помощью функции MPI_Type_create_struct и выполнить рассылку строк матрицы a[8][8] на 4 процесса в матрицу d[2][8] по следующей схеме:
* на нулевой процесс строки 0 и 4,
* на первый процесс строки 1 и 5,
* на второй процесс строки 2 и 6,
* на третий процесс строки 3 и 7.
Пример работы с функцией MPI_Type_create_struct см. здесь
https://docs.google.com/document/d/1vjykSQEAenjw85_jD9SIsJDyUXBUfX1IkUVHGnLOA0Q/edit?usp=sharing
3) На одном из процессов объявить и заполнить значениями верхнюю треугольную матрицу. Создать новый тип данных для передачи ненулевых элементов матрицы, используя конструктор MPI_Type_indexed. Выполнить передачу элементов на все процессы с помощью функции MPI_Bcast.
4) Пусть на нулевом процессе определены и заданы массив из десяти целых чисел и массив из восьми вещественных чисел. Упаковать два массива. Передать упакованные данные на все процессы. Распаковать на всех процессах принятые данные. Результат выдать на экран.

## (ч.7)
1) Создать новую группу из процессов с номерами 8, 3, 9, 1, 6. Используя конструктор MPI_Comm_create(…), создать коммуникатор группы. На нулевом процессе созданной группы объявить и заполнить числовыми значениями одномерный массив вещественных чисел и вывести его на экран. Используя функцию MPI_Bcast передать массив всем процессам созданной группы. Полученные массивы вывести на экран. Передать полученный массив из последнего процесса новой группы на нулевой процесс исходной группы. Выполнить программу на 10 процессах.
2) На основе исходной группы, создать 4 новые группы процессов со своим коммуникатором. На каждом процессе каждой группы объявить одномерный массив целых чисел и заполнить его числовыми значениями, равными номеру группы. Используя коллективные операции, собрать локальные массивы на нулевых   процессах каждой группы. Полученные массивы выдать на экран. Используя интеркоммуникатор, выполнить обмен собранными массивами между     первой – второй и третьей - четвертой группами.  Полученные массивы выдать на экран. Выполнить программу на 12 процессах.	Пример работы с интеркоммуникатором см. здесь
 https://docs.google.com/document/d/1YCgge5JyRaCeWO6pNF_aRN8XqF6S7WBin5nA9-KwDAg/edit?usp=sharing

## (ч.8)
1) Найти среднее арифметическое положительных чисел массива
2) Выполнить реализацию каскадной схемы (схемы сдваивания) для вычисления суммы последовательности чисел.
3) Написать программу вычисления числа π методом Монте-Карло. Суть метода в следующем. Возьмем круг радиуса 1, тогда площадь круга равна π, а площадь квадрата вокруг круга равна 4. Следовательно, отношение площади круга к площади квадрата равно π/4. Случайным образом генерируем точки в пределах контура квадрата, тогда отношение числа точек, попавших в круг, к общему количеству сгенерированных точек даст величину π/4. Сгенерированная точка находится в круге, если ее координаты соответствуют выражению x2+ y2<1. 
4) Написать программу сортировки массива методом чет-нечетной сортировки.
5) Написать программу вычисления определенного интеграла по квадратурной формуле Симпсона.
